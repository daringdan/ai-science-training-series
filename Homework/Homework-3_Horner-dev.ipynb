{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to AI-driven Science on Supercomputers\n",
    "## Week 3 Homework\n",
    "\n",
    "#### Dan Horner (danhorner@berkeley.edu)Â¶\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving CIFAR-10 dataset classification with CNNs\n",
    "\n",
    "## CIFAR-10 data set\n",
    "In this homework, we use the CIFAR-10 data set, which contains 32x32 color images from 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "\n",
    "The original training image data (x_train_orig) is a 3rd-order tensor of size (50000, 32, 32), i.e. it consists of 50000 images of size 32x32 pixels, while y_train_orig is a 50000-dimensional vector containing the correct classes ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') for each training sample.\n",
    "\n",
    "Since we are trying to evaluate performance of different models, we will be using a validation data set taken from the original training data set (80% train & 20% validation.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from image_dataset_loader import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 data loaded: train: 50000 test: 10000\n",
      "X_train: (40000, 32, 32, 3)\n",
      "y_train: (40000,)\n",
      "X_val: (10000, 32, 32, 3)\n",
      "y_val: (10000,)\n",
      "X_test: (10000, 32, 32, 3)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Data import copied from in-class notebook, and adapted for Train/Validation/Test\n",
    "\n",
    "(x_train_orig, y_train_orig), (x_test, y_test) = load('../03_neural_networks_tensorflow/cifar10', ['train', 'test'])\n",
    "\n",
    "x_train_orig = x_train_orig.astype(np.float32)\n",
    "x_test  = x_test.astype(np.float32)\n",
    "\n",
    "# Normalize values [0-1]\n",
    "x_train_orig /= 255.\n",
    "x_test  /= 255.\n",
    "\n",
    "y_train_orig = y_train_orig.astype(np.int32)\n",
    "y_test  = y_test.astype(np.int32)\n",
    "\n",
    "#Train / validation split\n",
    "x_train_i, x_val, y_train_i, y_val = train_test_split(x_train_orig, y_train_orig, test_size=0.2)\n",
    "\n",
    "print('CIFAR-10 data loaded: train:',len(x_train_orig),'test:',len(x_test))\n",
    "print('X_train:', x_train_i.shape)\n",
    "print('y_train:', y_train_i.shape)\n",
    "print('X_val:', x_val.shape)\n",
    "print('y_val:', y_val.shape)\n",
    "print('X_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Classifier(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, activation=tf.nn.tanh, dropout=(0.25, 0.50), hl = (32, 64, 128)):\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(hl[0], [3, 3], activation='relu')\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(hl[1], [3, 3], activation='relu')\n",
    "        self.pool_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.drop_4 = tf.keras.layers.Dropout(dropout[0])\n",
    "        self.dense_5 = tf.keras.layers.Dense(hl[2], activation='relu')\n",
    "        self.drop_6 = tf.keras.layers.Dropout(dropout[1])\n",
    "        self.dense_7 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_3(x)\n",
    "        x = self.drop_4(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = self.dense_5(x)\n",
    "        x = self.drop_6(x)\n",
    "        x = self.dense_7(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_concise(_x_train, _y_train, _batch_size, _n_training_epochs, _lr, _dropout, _hl):\n",
    "    model = CIFAR10Classifier(dropout = _dropout, hl =_hl)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])    \n",
    "    history = model.fit(_x_train, _y_train, batch_size=_batch_size, epochs=_n_training_epochs, verbose=2)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter run: 1 / 8\n",
      "128 0.1 (0.5, 0.25) (32, 32, 32)\n",
      "Epoch 1/3\n",
      "313/313 - 32s - loss: 1.7650 - accuracy: 0.3538 - 32s/epoch - 103ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 31s - loss: 1.4859 - accuracy: 0.4606 - 31s/epoch - 100ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 30s - loss: 1.3795 - accuracy: 0.5073 - 30s/epoch - 97ms/step\n",
      "[1.2472456693649292, 0.5641999840736389]\n",
      "Hyperparameter run: 2 / 8\n",
      "128 0.1 (0.5, 0.25) (32, 32, 64)\n",
      "Epoch 1/3\n",
      "313/313 - 32s - loss: 1.7040 - accuracy: 0.3764 - 32s/epoch - 102ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 31s - loss: 1.3908 - accuracy: 0.4999 - 31s/epoch - 98ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 31s - loss: 1.2619 - accuracy: 0.5507 - 31s/epoch - 100ms/step\n",
      "[1.1363106966018677, 0.6004999876022339]\n",
      "Hyperparameter run: 3 / 8\n",
      "128 0.1 (0.5, 0.25) (32, 64, 32)\n",
      "Epoch 1/3\n",
      "313/313 - 48s - loss: 1.8064 - accuracy: 0.3336 - 48s/epoch - 152ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 46s - loss: 1.4924 - accuracy: 0.4547 - 46s/epoch - 148ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 47s - loss: 1.3745 - accuracy: 0.4961 - 47s/epoch - 149ms/step\n",
      "[1.1856718063354492, 0.590499997138977]\n",
      "Hyperparameter run: 4 / 8\n",
      "128 0.1 (0.5, 0.25) (32, 64, 64)\n",
      "Epoch 1/3\n",
      "313/313 - 48s - loss: 1.6737 - accuracy: 0.3961 - 48s/epoch - 155ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 48s - loss: 1.3356 - accuracy: 0.5210 - 48s/epoch - 153ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 48s - loss: 1.2060 - accuracy: 0.5728 - 48s/epoch - 154ms/step\n",
      "[1.088107705116272, 0.6215999722480774]\n",
      "Hyperparameter run: 5 / 8\n",
      "128 0.1 (0.5, 0.25) (64, 32, 32)\n",
      "Epoch 1/3\n",
      "313/313 - 53s - loss: 1.8272 - accuracy: 0.3253 - 53s/epoch - 170ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 52s - loss: 1.5373 - accuracy: 0.4363 - 52s/epoch - 167ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 55s - loss: 1.4249 - accuracy: 0.4828 - 55s/epoch - 175ms/step\n",
      "[1.24783456325531, 0.5665000081062317]\n",
      "Hyperparameter run: 6 / 8\n",
      "128 0.1 (0.5, 0.25) (64, 32, 64)\n",
      "Epoch 1/3\n",
      "313/313 - 57s - loss: 1.7246 - accuracy: 0.3742 - 57s/epoch - 183ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 56s - loss: 1.3974 - accuracy: 0.4991 - 56s/epoch - 179ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 56s - loss: 1.2707 - accuracy: 0.5458 - 56s/epoch - 180ms/step\n",
      "[1.1187211275100708, 0.6111000180244446]\n",
      "Hyperparameter run: 7 / 8\n",
      "128 0.1 (0.5, 0.25) (64, 64, 32)\n",
      "Epoch 1/3\n",
      "313/313 - 73s - loss: 1.8283 - accuracy: 0.3132 - 73s/epoch - 233ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 71s - loss: 1.5573 - accuracy: 0.4193 - 71s/epoch - 226ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 71s - loss: 1.4485 - accuracy: 0.4611 - 71s/epoch - 228ms/step\n",
      "[1.255596399307251, 0.5637000203132629]\n",
      "Hyperparameter run: 8 / 8\n",
      "128 0.1 (0.5, 0.25) (64, 64, 64)\n",
      "Epoch 1/3\n",
      "313/313 - 75s - loss: 1.6952 - accuracy: 0.3803 - 75s/epoch - 241ms/step\n",
      "Epoch 2/3\n",
      "313/313 - 75s - loss: 1.3598 - accuracy: 0.5135 - 75s/epoch - 241ms/step\n",
      "Epoch 3/3\n",
      "313/313 - 76s - loss: 1.2391 - accuracy: 0.5577 - 76s/epoch - 243ms/step\n",
      "[1.1580592393875122, 0.5996000170707703]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "bs_vals = (128,)#(128, 256, 512)\n",
    "lr_vals = (0.1,)#(0.01, 0.05, 0.1)\n",
    "do_vals = (0.10, 0.25, 0.50)\n",
    "hl_vals = (32, 64)\n",
    "\n",
    "n_runs = len(bs_vals) * len(lr_vals) * len(hl_vals) * len(hl_vals) * len(hl_vals)\n",
    "\n",
    "runs_li = []\n",
    "i = 0\n",
    "for bs in bs_vals:\n",
    "    for lr in lr_vals:\n",
    "        for do0 in (0.50,):\n",
    "            for do1 in (0.25,):\n",
    "                for hl0 in hl_vals:\n",
    "                    for hl1 in hl_vals:\n",
    "                        for hl2 in hl_vals:\n",
    "                            do = (do0, do1)\n",
    "                            hl = (hl0, hl1, hl2)\n",
    "                            i += 1\n",
    "                            print('Hyperparameter run:', i, '/', n_runs)\n",
    "                            print(bs, lr, do, hl)\n",
    "                            history_i, model_i = train_network_concise(x_train_i, y_train_i, bs, epochs, lr, do, hl)\n",
    "                            scores = model_i.evaluate(x_val, y_val, verbose=0)\n",
    "                            print(scores)\n",
    "                            runs_li.append({'scores': scores, 'model': model_i, 'bs': bs, 'lr': lr, 'do': do, 'hl': hl})\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 0.6464999914169312\n",
      "128\n",
      "0.1\n",
      "(0.5, 0.25)\n",
      "0.6040999889373779\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "i_best = 0\n",
    "val_acc_best = 0\n",
    "for r in runs_li:\n",
    "    val_acc = r['scores'][1]\n",
    "    if(val_acc > val_acc_best):\n",
    "        i_best = i\n",
    "        val_acc_best = val_acc\n",
    "    i += 1\n",
    "print(i_best, val_acc_best)\n",
    "print(runs_li[i_best]['bs'])\n",
    "print(runs_li[i_best]['lr'])\n",
    "print(runs_li[i_best]['do'])\n",
    "print(runs_li[i_best]['hl'])\n",
    "\n",
    "scores_test = model_i.evaluate(x_test, y_test, verbose=0)\n",
    "print(scores_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, x, y_true):\n",
    "    y_pred = np.argmax(model.predict(x), axis = 1)\n",
    "    N = y_pred.shape[0]\n",
    "    acc = (y_true == y_pred).sum() / N\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6083"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    # if labels are integers, use sparse categorical crossentropy\n",
    "    # network's final layer is softmax, so from_logtis=False\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "   \n",
    "    return scce(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, batch_data, y_true):\n",
    "    y_pred = model(batch_data)\n",
    "    loss = compute_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function that will manage the training loop for us:\n",
    "\n",
    "def train_loop(batch_size, n_training_epochs, model, opt):\n",
    "    \n",
    "    @tf.function()\n",
    "    def train_iteration(data, y_true, model, opt):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = forward_pass(model, data, y_true)\n",
    "\n",
    "        trainable_vars = model.trainable_variables\n",
    "\n",
    "        # Apply the update to the network (one at a time):\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        opt.apply_gradients(zip(grads, trainable_vars))\n",
    "        return loss\n",
    "\n",
    "    for i_epoch in range(n_training_epochs):\n",
    "        print(\"beginning epoch %d\" % i_epoch)\n",
    "        start = time.time()\n",
    "\n",
    "        epoch_steps = int(40000/batch_size)\n",
    "        dataset.shuffle(40000) # Shuffle the whole dataset in memory\n",
    "        batches = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "        for i_batch, (batch_data, y_true) in enumerate(batches):\n",
    "            batch_data = tf.reshape(batch_data, [-1, 32, 32, 3])\n",
    "            loss = train_iteration(batch_data, y_true, model, opt)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"took %1.1f seconds for epoch #%d\" % (end-start, i_epoch))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: improve the accuracy of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update this notebook to ensure more accuracy. How high can it be raised? Changes like increasing the number of epochs, altering the learning weight, altering the number of neurons the hidden layer, chnaging the optimizer, etc. could be made directly in the notebook. You can also change the model specification by expanding the network's layer. The current notebook's training accuracy is roughly 58.69%, although it varies randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
